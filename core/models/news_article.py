#!/usr/bin/env python3

import json
import logging
from datetime import datetime
from enum import Enum
from typing import Any, Optional, Union

import aiohttp
from bs4 import BeautifulSoup
from newspaper import Article
from pydantic import BaseModel, ConfigDict, Field

logger = logging.getLogger(__name__)


class ArticleStatus(str, Enum):
    """æ–‡ç« çŠ¶æ€æšä¸¾"""

    PENDING = "pending"  # å¾…å¤„ç†
    PROCESSING = "processing"  # å¤„ç†ä¸­
    PROCESSED = "processed"  # å·²å¤„ç†
    FAILED = "failed"  # å¤„ç†å¤±è´¥
    ARCHIVED = "archived"  # å·²å½’æ¡£


class NewsArticleBase(BaseModel):
    """æ–°é—»æ–‡ç« åŸºç¡€æ¨¡å‹ï¼ŒåŒ…å«å…±æœ‰å­—æ®µ"""

    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜")
    url: str = Field(..., description="æ–‡ç« URL")
    url_hash: str = Field(..., description="URLå“ˆå¸Œï¼ˆå»é‡ç”¨ï¼‰")
    content: Optional[str] = Field(default=None, description="æ–‡ç« æ­£æ–‡å†…å®¹ï¼ˆæ•°æ®åº“ä¸­å¯èƒ½ä¸ºNULLï¼‰")
    summary: Optional[str] = Field(default=None, description="æ–‡ç« æ‘˜è¦")
    author: Optional[str] = Field(default=None, description="ä½œè€…")
    source_id: int = Field(..., description="æ–°é—»æºID")
    source_name: Optional[str] = Field(default=None, description="æ¥æºåç§°")
    category: Optional[str] = Field(default=None, description="æ–‡ç« åˆ†ç±»")
    published_at: Optional[datetime] = Field(default=None, description="å‘å¸ƒæ—¶é—´")
    crawled_at: Optional[datetime] = Field(
        default_factory=datetime.now, description="æŠ“å–æ—¶é—´"
    )
    language: str = Field(default="zh", description="è¯­è¨€ä»£ç ")
    region: str = Field(default="CN", description="åœ°åŒºä»£ç ")
    entities: Optional[list[dict[str, Any]]] = Field(default=None, description="å®ä½“è¯†åˆ«ç»“æœ")
    keywords: Optional[list[str]] = Field(default=None, description="å…³é”®è¯æå–ç»“æœ")
    sentiment_score: Optional[float] = Field(default=None, description="æƒ…æ„Ÿåˆ†æå¾—åˆ† (-1åˆ°1)")
    topics: Optional[list[dict[str, Any]]] = Field(default=None, description="ä¸»é¢˜åˆ†æç»“æœ")
    importance_score: float = Field(default=0.0, description="é‡è¦æ€§è¯„åˆ† (0-1)")
    market_relevance_score: float = Field(default=0.0, description="å¸‚åœºç›¸å…³æ€§è¯„åˆ† (0-1)")
    status: ArticleStatus = Field(default=ArticleStatus.PENDING, description="å¤„ç†çŠ¶æ€")
    processed_at: Optional[datetime] = Field(default=None, description="å¤„ç†å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(default=None, description="å¤„ç†é”™è¯¯ä¿¡æ¯")
    word_count: int = Field(default=0, description="å­—æ•°ç»Ÿè®¡")
    read_time_minutes: int = Field(default=0, description="é¢„è®¡é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")

    model_config = ConfigDict(from_attributes=True)


class NewsArticleCreate(BaseModel):
    """ç”¨äºåˆ›å»ºæ–°é—»æ–‡ç« çš„æ¨¡å‹"""

    title: str = Field(..., description="æ–‡ç« æ ‡é¢˜")
    url: str = Field(..., description="æ–‡ç« URL")
    url_hash: Optional[str] = Field(default=None, description="URLå“ˆå¸Œï¼ˆå»é‡ç”¨ï¼‰ï¼Œå¦‚æœä¸ºç©ºä¼šè‡ªåŠ¨ç”Ÿæˆ")
    content: Optional[str] = Field(default=None, description="æ–‡ç« æ­£æ–‡å†…å®¹")
    summary: Optional[str] = Field(default=None, description="æ–‡ç« æ‘˜è¦")
    author: Optional[str] = Field(default=None, description="ä½œè€…")
    source_id: int = Field(..., description="æ–°é—»æºID")
    source_name: Optional[str] = Field(default=None, description="æ¥æºåç§°")
    category: Optional[str] = Field(default=None, description="æ–‡ç« åˆ†ç±»")
    published_at: Optional[datetime] = Field(default=None, description="å‘å¸ƒæ—¶é—´")
    crawled_at: Optional[datetime] = Field(
        default_factory=datetime.now, description="æŠ“å–æ—¶é—´"
    )
    language: str = Field(default="zh", description="è¯­è¨€ä»£ç ")
    region: str = Field(default="CN", description="åœ°åŒºä»£ç ")
    entities: Optional[list[dict[str, Any]]] = Field(default=None, description="å®ä½“è¯†åˆ«ç»“æœ")
    keywords: Optional[list[str]] = Field(default=None, description="å…³é”®è¯æå–ç»“æœ")
    sentiment_score: Optional[float] = Field(default=None, description="æƒ…æ„Ÿåˆ†æå¾—åˆ† (-1åˆ°1)")
    topics: Optional[list[dict[str, Any]]] = Field(default=None, description="ä¸»é¢˜åˆ†æç»“æœ")
    importance_score: float = Field(default=0.0, description="é‡è¦æ€§è¯„åˆ† (0-1)")
    market_relevance_score: float = Field(default=0.0, description="å¸‚åœºç›¸å…³æ€§è¯„åˆ† (0-1)")
    status: ArticleStatus = Field(default=ArticleStatus.PENDING, description="å¤„ç†çŠ¶æ€")
    processed_at: Optional[datetime] = Field(default=None, description="å¤„ç†å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(default=None, description="å¤„ç†é”™è¯¯ä¿¡æ¯")
    word_count: int = Field(default=0, description="å­—æ•°ç»Ÿè®¡")
    read_time_minutes: int = Field(default=0, description="é¢„è®¡é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")

    model_config = ConfigDict(from_attributes=True)


class NewsArticleUpdate(BaseModel):
    """ç”¨äºæ›´æ–°æ–°é—»æ–‡ç« çš„æ¨¡å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½æ˜¯å¯é€‰çš„"""

    title: Optional[str] = Field(default=None, description="æ–‡ç« æ ‡é¢˜")
    url: Optional[str] = Field(default=None, description="æ–‡ç« URL")
    url_hash: Optional[str] = Field(default=None, description="URLå“ˆå¸Œï¼ˆå»é‡ç”¨ï¼‰")
    content: Optional[str] = Field(default=None, description="æ–‡ç« æ­£æ–‡å†…å®¹")
    summary: Optional[str] = Field(default=None, description="æ–‡ç« æ‘˜è¦")
    author: Optional[str] = Field(default=None, description="ä½œè€…")
    source_id: Optional[int] = Field(default=None, description="æ–°é—»æºID")
    source_name: Optional[str] = Field(default=None, description="æ¥æºåç§°")
    category: Optional[str] = Field(default=None, description="æ–‡ç« åˆ†ç±»")
    published_at: Optional[datetime] = Field(default=None, description="å‘å¸ƒæ—¶é—´")
    crawled_at: Optional[datetime] = Field(default=None, description="æŠ“å–æ—¶é—´")
    language: Optional[str] = Field(default=None, description="è¯­è¨€ä»£ç ")
    region: Optional[str] = Field(default=None, description="åœ°åŒºä»£ç ")
    entities: Optional[list[dict[str, Any]]] = Field(default=None, description="å®ä½“è¯†åˆ«ç»“æœ")
    keywords: Optional[list[str]] = Field(default=None, description="å…³é”®è¯æå–ç»“æœ")
    sentiment_score: Optional[float] = Field(default=None, description="æƒ…æ„Ÿåˆ†æå¾—åˆ† (-1åˆ°1)")
    topics: Optional[list[dict[str, Any]]] = Field(default=None, description="ä¸»é¢˜åˆ†æç»“æœ")
    importance_score: Optional[float] = Field(default=None, description="é‡è¦æ€§è¯„åˆ† (0-1)")
    market_relevance_score: Optional[float] = Field(
        default=None, description="å¸‚åœºç›¸å…³æ€§è¯„åˆ† (0-1)"
    )
    status: Optional[ArticleStatus] = Field(default=None, description="å¤„ç†çŠ¶æ€")
    processed_at: Optional[datetime] = Field(default=None, description="å¤„ç†å®Œæˆæ—¶é—´")
    error_message: Optional[str] = Field(default=None, description="å¤„ç†é”™è¯¯ä¿¡æ¯")
    word_count: Optional[int] = Field(default=None, description="å­—æ•°ç»Ÿè®¡")
    read_time_minutes: Optional[int] = Field(default=None, description="é¢„è®¡é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")

    model_config = ConfigDict(from_attributes=True)


class NewsArticle(NewsArticleBase):
    """å®Œæ•´çš„æ–°é—»æ–‡ç« æ¨¡å‹ï¼ŒåŒ…å«ID"""

    id: int = Field(..., description="ID")
    created_at: Optional[datetime] = Field(
        default_factory=datetime.now, description="åˆ›å»ºæ—¶é—´"
    )
    updated_at: Optional[datetime] = Field(
        default_factory=datetime.now, description="æ›´æ–°æ—¶é—´"
    )

    model_config = ConfigDict(from_attributes=True)

    async def get_content_if_missing(
        self, force_refresh: bool = False
    ) -> Optional[str]:
        """
        ğŸ”¥ åŠ¨æ€è·å–æ–°é—»å†…å®¹ï¼ˆå¦‚æœæ•°æ®åº“ä¸­ç¼ºå¤±æˆ–éœ€è¦åˆ·æ–°ï¼‰

        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å†…å®¹ï¼Œå³ä½¿å·²å­˜åœ¨

        Returns:
            è·å–åˆ°çš„å†…å®¹æˆ–None
        """
        # å¦‚æœå·²æœ‰å†…å®¹ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œç›´æ¥è¿”å›
        if self.content and self.content.strip() and not force_refresh:
            return self.content

        logger.info(f"å¼€å§‹è·å–æ–°é—»å†…å®¹: {self.url}")

        try:
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨newspaper3k
            content = await self._fetch_with_newspaper()
            if content:
                # æ›´æ–°å½“å‰å¯¹è±¡çš„contentå­—æ®µ
                self.content = content
                self.word_count = len(content)
                self.read_time_minutes = max(1, len(content) // 200)
                logger.info(f"âœ… ä½¿ç”¨newspaper3kè·å–å†…å®¹æˆåŠŸ: {len(content)}å­—ç¬¦")
                return content

            # æ–¹æ³•2: å›é€€åˆ°requests + BeautifulSoup
            content = await self._fetch_with_requests()
            if content:
                self.content = content
                self.word_count = len(content)
                self.read_time_minutes = max(1, len(content) // 200)
                logger.info(f"âœ… ä½¿ç”¨requestsè·å–å†…å®¹æˆåŠŸ: {len(content)}å­—ç¬¦")
                return content

            logger.warning(f"âš ï¸ æ— æ³•è·å–æ–°é—»å†…å®¹: {self.url}")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å–æ–°é—»å†…å®¹å¤±è´¥: {self.url} - {e}")
            return None

    async def _fetch_with_newspaper(self) -> Optional[str]:
        """ä½¿ç”¨newspaper3kè·å–å†…å®¹"""
        try:
            article = Article(self.url, language="zh")
            article.download()
            article.parse()

            if article.text and len(article.text.strip()) > 100:
                return article.text.strip()
            return None

        except Exception as e:
            logger.debug(f"newspaper3kè·å–å¤±è´¥: {e}")
            return None

    async def _fetch_with_requests(self) -> Optional[str]:
        """ä½¿ç”¨requests + BeautifulSoupè·å–å†…å®¹"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }

            # ä½¿ç”¨aiohttpå¼‚æ­¥è¯·æ±‚
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    self.url, headers=headers, timeout=10
                ) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, "html.parser")

                        # å°è¯•å¤šç§å†…å®¹æå–ç­–ç•¥
                        content = self._extract_content_from_soup(soup)
                        if content and len(content.strip()) > 100:
                            return content.strip()

            return None

        except Exception as e:
            logger.debug(f"requestsè·å–å¤±è´¥: {e}")
            return None

    def _extract_content_from_soup(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ–‡ç« å†…å®¹"""
        try:
            # ç­–ç•¥1: å°è¯•å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                "article",
                ".article-content",
                ".post-content",
                ".content",
                ".main-content",
                '[class*="content"]',
                "main",
                ".article-body",
                ".post-body",
            ]

            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(strip=True)
                    if len(content) > 100:
                        return content

            # ç­–ç•¥2: å›é€€åˆ°bodyä¸­æœ€é•¿çš„æ–‡æœ¬æ®µè½
            paragraphs = soup.find_all(["p", "div"])
            longest_text = ""
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > len(longest_text):
                    longest_text = text

            if len(longest_text) > 100:
                return longest_text

            return None

        except Exception as e:
            logger.debug(f"å†…å®¹æå–å¤±è´¥: {e}")
            return None

    def get_analysis_content(self) -> str:
        """
        ğŸ”¥ è·å–ç”¨äºåˆ†æçš„å†…å®¹æ–‡æœ¬

        Returns:
            ç”¨äºåˆ†æçš„å®Œæ•´æ–‡æœ¬ï¼ˆæ ‡é¢˜+æ‘˜è¦+å†…å®¹ï¼‰
        """
        content_parts = []

        # ä¼˜å…ˆçº§1: æ ‡é¢˜ï¼ˆæƒé‡æœ€é«˜ï¼‰
        if self.title:
            content_parts.append(f"ã€æ ‡é¢˜ã€‘{self.title}")

        # ä¼˜å…ˆçº§2: æ‘˜è¦
        if self.summary:
            content_parts.append(f"ã€æ‘˜è¦ã€‘{self.summary}")

        # ä¼˜å…ˆçº§3: æ­£æ–‡å†…å®¹
        if self.content:
            # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…tokenè¶…é™
            content_text = (
                self.content[:3000] if len(self.content) > 3000 else self.content
            )
            content_parts.append(f"ã€æ­£æ–‡ã€‘{content_text}")
        else:
            # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œè‡³å°‘ä½¿ç”¨URLä½œä¸ºå‚è€ƒ
            content_parts.append(f"ã€æ¥æºã€‘{self.url}")

        # ä¼˜å…ˆçº§4: å…³é”®è¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.keywords and isinstance(self.keywords, list):
            keywords_text = "ã€".join(self.keywords[:10])  # å–å‰10ä¸ªå…³é”®è¯
            content_parts.append(f"ã€å…³é”®è¯ã€‘{keywords_text}")

        # ä¼˜å…ˆçº§5: å®ä½“ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.entities and isinstance(self.entities, list):
            entities_text = []
            for entity in self.entities[:5]:  # å–å‰5ä¸ªå®ä½“
                if isinstance(entity, dict) and "name" in entity:
                    entities_text.append(entity["name"])
            if entities_text:
                content_parts.append(f"ã€å®ä½“ã€‘{', '.join(entities_text)}")

        return "\n\n".join(content_parts)


# ç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„è¾…åŠ©å‡½æ•°
def news_article_to_dict(
    article: Union[NewsArticle, NewsArticleCreate, NewsArticleUpdate]
) -> dict:
    """
    å°†NewsArticleæ¨¡å‹è½¬æ¢ä¸ºå­—å…¸ï¼Œç”¨äºæ•°æ®åº“æ“ä½œ

    Args:
        article: NewsArticleã€NewsArticleCreateæˆ–NewsArticleUpdateæ¨¡å‹

    Returns:
        å­—å…¸è¡¨ç¤º
    """
    result = {}

    if isinstance(article, (NewsArticleCreate, NewsArticleUpdate)):
        result = article.model_dump(exclude_unset=True, exclude_none=True)
    else:
        result = article.model_dump(exclude_none=True)

    # å¤„ç†æšä¸¾å­—æ®µ
    if "status" in result and isinstance(result["status"], ArticleStatus):
        result["status"] = result["status"].value

    # å¤„ç†JSONå­—æ®µ
    json_fields = ["entities", "keywords", "topics"]
    for field in json_fields:
        if field in result and result[field] is not None:
            result[field] = json.dumps(result[field], ensure_ascii=False)

    return result


def dict_to_news_article(data: dict) -> NewsArticle:
    """
    å°†å­—å…¸è½¬æ¢ä¸ºNewsArticleæ¨¡å‹ï¼Œå¤„ç†å„ç§æ•°æ®ç±»å‹è½¬æ¢å’Œé»˜è®¤å€¼

    Args:
        data: æ¥è‡ªæ•°æ®åº“çš„å­—å…¸æ•°æ®

    Returns:
        NewsArticleæ¨¡å‹å®ä¾‹
    """
    # åˆ›å»ºæ•°æ®çš„å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    processed_data = data.copy()

    # å¤„ç†æšä¸¾å­—æ®µ
    if "status" in processed_data:
        status = processed_data["status"]
        if isinstance(status, str):
            try:
                processed_data["status"] = ArticleStatus(status)
            except ValueError:
                processed_data["status"] = ArticleStatus.PENDING

    # å¤„ç†JSONå­—æ®µ
    json_fields = ["entities", "keywords", "topics"]
    for json_field in json_fields:
        if json_field in processed_data:
            json_value = processed_data[json_field]

            # å¦‚æœæ˜¯Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œè®¾ä¸ºNone
            if json_value is None or (
                isinstance(json_value, str) and not json_value.strip()
            ):
                processed_data[json_field] = None
            # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨æˆ–å­—å…¸å¯¹è±¡ï¼Œä¿æŒä¸å˜
            elif isinstance(json_value, (list, dict)):
                pass
            # å°è¯•è§£æJSONå­—ç¬¦ä¸²
            elif isinstance(json_value, str):
                try:
                    processed_data[json_field] = json.loads(json_value)
                except (ValueError, json.JSONDecodeError):
                    processed_data[json_field] = None

    # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨å¹¶æœ‰é»˜è®¤å€¼
    processed_data["importance_score"] = float(
        processed_data.get("importance_score", 0.0) or 0.0
    )
    processed_data["market_relevance_score"] = float(
        processed_data.get("market_relevance_score", 0.0) or 0.0
    )
    processed_data["word_count"] = int(processed_data.get("word_count", 0) or 0)
    processed_data["read_time_minutes"] = int(
        processed_data.get("read_time_minutes", 0) or 0
    )
    processed_data["language"] = processed_data.get("language", "zh") or "zh"
    processed_data["region"] = processed_data.get("region", "CN") or "CN"

    # å¤„ç†æµ®ç‚¹æ•°å­—æ®µ
    if (
        "sentiment_score" in processed_data
        and processed_data["sentiment_score"] is not None
    ):
        try:
            processed_data["sentiment_score"] = float(processed_data["sentiment_score"])
        except (ValueError, TypeError):
            processed_data["sentiment_score"] = None

    # å¤„ç†æ—¥æœŸå­—æ®µ
    date_fields = [
        "published_at",
        "crawled_at",
        "processed_at",
        "created_at",
        "updated_at",
    ]
    for date_field in date_fields:
        if date_field in processed_data:
            date_value = processed_data[date_field]

            # å¦‚æœæ˜¯Noneæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œè®¾ä¸ºNoneï¼ˆé™¤äº†crawled_atï¼‰
            if date_value is None or (
                isinstance(date_value, str) and not date_value.strip()
            ):
                if date_field == "crawled_at":
                    processed_data[date_field] = datetime.now()
                else:
                    processed_data[date_field] = None
                continue

            # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡ï¼Œä¿æŒä¸å˜
            if isinstance(date_value, datetime):
                continue

            # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetime
            if isinstance(date_value, str):
                try:
                    # å°è¯•ISOæ ¼å¼
                    processed_data[date_field] = datetime.fromisoformat(
                        date_value.replace("Z", "+00:00")
                    )
                except (ValueError, AttributeError):
                    try:
                        # å°è¯•æ ‡å‡†MySQLæ ¼å¼
                        processed_data[date_field] = datetime.strptime(
                            date_value, "%Y-%m-%d %H:%M:%S"
                        )
                    except (ValueError, AttributeError):
                        if date_field == "crawled_at":
                            processed_data[date_field] = datetime.now()
                        else:
                            processed_data[date_field] = None

    try:
        # åˆ›å»ºNewsArticleæ¨¡å‹å®ä¾‹
        return NewsArticle(**processed_data)
    except Exception as e:
        print(f"æ— æ³•åˆ›å»ºNewsArticleæ¨¡å‹: {e}")
        print(f"æ•°æ®: {processed_data}")

        # è¿”å›å¸¦æœ‰æœ€å°å¿…è¦å­—æ®µçš„NewsArticleï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
        return NewsArticle(
            id=processed_data.get("id", 0),
            title=processed_data.get("title", "Unknown Title"),
            url=processed_data.get("url", "http://example.com"),
            url_hash=processed_data.get("url_hash", "unknown_hash"),
            source_id=processed_data.get("source_id", 0),
            status=ArticleStatus.PENDING,
        )
