#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®Œæ•´æ–°é—»èšåˆæµ‹è¯•
æµ‹è¯•RSSå’ŒAPIæ–°é—»æºçš„æŠ“å–åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®åº“æ“ä½œ
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.news_aggregator.rss_aggregator import RSSAggregator
from core.news_aggregator.api_aggregator import APIAggregator
from core.handler.news_source_handler import NewsSourceHandler
from core.handler.news_article_handler import NewsArticleHandler
from core.models.news_source import NewsSource, NewsSourceType, NewsSourceStatus

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_rss_sources():
    """æµ‹è¯•RSSæ–°é—»æº"""
    logger.info("ğŸ” å¼€å§‹æµ‹è¯•RSSæ–°é—»æº...")
    
    # è·å–æ´»è·ƒçš„RSSæº
    source_handler = NewsSourceHandler()
    sources = await source_handler.get_active_news_sources()
    rss_sources = [s for s in sources if s.source_type == NewsSourceType.RSS]
    
    if not rss_sources:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„RSSæº")
        return 0
    
    success_count = 0
    total_articles = 0
    
    async with RSSAggregator() as aggregator:
        for source in rss_sources[:5]:  # åªæµ‹è¯•å‰5ä¸ªæº
            try:
                logger.info(f"æµ‹è¯•RSSæº: {source.name}")
                articles = await aggregator.fetch_rss_feed(source)
                
                if articles:
                    logger.info(f"  âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
                    success_count += 1
                    total_articles += len(articles)
                    
                    # æ˜¾ç¤ºéƒ¨åˆ†æ–‡ç« æ ‡é¢˜
                    for i, article in enumerate(articles[:3]):
                        title = article.get('title', 'æ— æ ‡é¢˜')[:60]
                        logger.info(f"    {i+1}. {title}...")
                else:
                    logger.warning(f"  âš ï¸  æœªè·å–åˆ°æ–‡ç« ")
                    
            except Exception as e:
                logger.error(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            
            # é—´éš”é¿å…è¯·æ±‚è¿‡é¢‘
            await asyncio.sleep(1)
    
    logger.info(f"ğŸ“Š RSSæºæµ‹è¯•å®Œæˆ: æˆåŠŸ {success_count}/{len(rss_sources)} ä¸ªæºï¼Œå…± {total_articles} ç¯‡æ–‡ç« ")
    return total_articles

async def test_api_sources():
    """æµ‹è¯•APIæ–°é—»æº"""
    logger.info("ğŸ” å¼€å§‹æµ‹è¯•APIæ–°é—»æº...")
    
    # è·å–æ´»è·ƒçš„APIæº
    source_handler = NewsSourceHandler()
    sources = await source_handler.get_all_news_sources()
    api_sources = [s for s in sources if s.source_type == NewsSourceType.API and s.status == NewsSourceStatus.ACTIVE]
    
    if not api_sources:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„APIæº")
        return 0
    
    success_count = 0
    total_articles = 0
    
    async with APIAggregator() as aggregator:
        for source in api_sources:
            try:
                logger.info(f"æµ‹è¯•APIæº: {source.name}")
                articles = await aggregator.fetch_api_feed(source)
                
                if articles:
                    logger.info(f"  âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
                    success_count += 1
                    total_articles += len(articles)
                    
                    # æ˜¾ç¤ºéƒ¨åˆ†æ–‡ç« æ ‡é¢˜
                    for i, article in enumerate(articles[:3]):
                        title = article.get('title', 'æ— æ ‡é¢˜')[:60]
                        logger.info(f"    {i+1}. {title}...")
                else:
                    logger.warning(f"  âš ï¸  æœªè·å–åˆ°æ–‡ç« ")
                    
            except Exception as e:
                logger.error(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            
            # é—´éš”é¿å…è¯·æ±‚è¿‡é¢‘
            await asyncio.sleep(2)
    
    logger.info(f"ğŸ“Š APIæºæµ‹è¯•å®Œæˆ: æˆåŠŸ {success_count}/{len(api_sources)} ä¸ªæºï¼Œå…± {total_articles} ç¯‡æ–‡ç« ")
    return total_articles

async def test_database_operations():
    """æµ‹è¯•æ•°æ®åº“æ“ä½œ"""
    logger.info("ğŸ” å¼€å§‹æµ‹è¯•æ•°æ®åº“æ“ä½œ...")
    
    try:
        # æµ‹è¯•æ–°é—»æºæ“ä½œ
        source_handler = NewsSourceHandler()
        
        # åˆ›å»ºæµ‹è¯•æ–°é—»æº
        test_source_data = {
            "name": "æµ‹è¯•RSSæº",
            "description": "ç”¨äºæµ‹è¯•çš„RSSæ–°é—»æº",
            "source_type": NewsSourceType.RSS,
            "url": "https://cn.investing.com/rss/news.rss",
            "update_frequency": 3600,
            "max_articles_per_fetch": 5,
            "language": "zh",
            "region": "CN",
            "status": NewsSourceStatus.ACTIVE
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = await source_handler.get_news_source_by_name(test_source_data["name"])
        if existing:
            logger.info("  æµ‹è¯•æºå·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
            test_source = existing
        else:
            test_source = await source_handler.create_news_source(test_source_data)
            if test_source:
                logger.info(f"  âœ… åˆ›å»ºæµ‹è¯•æ–°é—»æºæˆåŠŸ (ID: {test_source.id})")
            else:
                logger.error("  âŒ åˆ›å»ºæµ‹è¯•æ–°é—»æºå¤±è´¥")
                return False
        
        # æµ‹è¯•æ–°é—»æ–‡ç« æ“ä½œ
        article_handler = NewsArticleHandler()
        
        # æ¨¡æ‹ŸæŠ“å–æ–‡ç« å¹¶ä¿å­˜
        test_article_data = {
            "title": "æµ‹è¯•æ–°é—»æ ‡é¢˜",
            "url": "https://example.com/test-news",
            "url_hash": "test_hash_123",
            "content": "è¿™æ˜¯ä¸€ç¯‡æµ‹è¯•æ–°é—»çš„å†…å®¹...",
            "summary": "æµ‹è¯•æ–°é—»æ‘˜è¦",
            "source_id": test_source.id,
            "source_name": test_source.name,
            "language": "zh",
            "region": "CN"
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_article = await article_handler.repository.get_article_by_url_hash(test_article_data["url_hash"])
        if existing_article:
            logger.info("  æµ‹è¯•æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
        else:
            test_article = await article_handler.create_news_article(test_article_data)
            if test_article:
                logger.info(f"  âœ… åˆ›å»ºæµ‹è¯•æ–‡ç« æˆåŠŸ (ID: {test_article.id})")
            else:
                logger.error("  âŒ åˆ›å»ºæµ‹è¯•æ–‡ç« å¤±è´¥")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        source_stats = await source_handler.get_news_source_stats()
        article_stats = await article_handler.get_article_stats()
        
        logger.info(f"  ğŸ“Š æ–°é—»æºç»Ÿè®¡: {source_stats}")
        logger.info(f"  ğŸ“Š æ–‡ç« ç»Ÿè®¡: {article_stats}")
        
        logger.info("âœ… æ•°æ®åº“æ“ä½œæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_full_workflow():
    """æµ‹è¯•å®Œæ•´çš„æ–°é—»èšåˆå·¥ä½œæµ"""
    logger.info("ğŸ” å¼€å§‹æµ‹è¯•å®Œæ•´å·¥ä½œæµ...")
    
    try:
        # è·å–ä¸€ä¸ªæ´»è·ƒçš„RSSæº
        source_handler = NewsSourceHandler()
        article_handler = NewsArticleHandler()
        
        active_sources = await source_handler.get_active_news_sources()
        rss_sources = [s for s in active_sources if s.source_type == NewsSourceType.RSS]
        
        if not rss_sources:
            logger.warning("âš ï¸  æ²¡æœ‰æ´»è·ƒçš„RSSæºå¯ä¾›æµ‹è¯•")
            return False
        
        test_source = rss_sources[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªRSSæº
        logger.info(f"ä½¿ç”¨æµ‹è¯•æº: {test_source.name}")
        
        # 1. æŠ“å–æ–°é—»
        async with RSSAggregator() as aggregator:
            articles = await aggregator.fetch_rss_feed(test_source)
            
        if not articles:
            logger.warning("âš ï¸  æœªæŠ“å–åˆ°æ–°é—»æ–‡ç« ")
            return False
        
        logger.info(f"âœ… æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # 2. æ‰¹é‡ä¿å­˜æ–‡ç« 
        created_articles = await article_handler.batch_create_articles(articles)
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(created_articles)} ç¯‡æ–‡ç« ")
        
        # 3. æ›´æ–°æ–°é—»æºçš„æŠ“å–ä¿¡æ¯
        success = await source_handler.update_source_fetch_result(
            test_source.id, True, None, len(created_articles)
        )
        if success:
            logger.info("âœ… æ›´æ–°æ–°é—»æºæŠ“å–ä¿¡æ¯æˆåŠŸ")
        
        # 4. æŸ¥è¯¢æœ€è¿‘çš„æ–‡ç« 
        recent_articles = await article_handler.get_recent_articles(1, 10)  # æœ€è¿‘1å°æ—¶ï¼Œ10ç¯‡
        logger.info(f"âœ… æŸ¥è¯¢åˆ° {len(recent_articles)} ç¯‡æœ€è¿‘æ–‡ç« ")
        
        logger.info("ğŸ‰ å®Œæ•´å·¥ä½œæµæµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å®Œæ•´å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ–°é—»èšåˆç³»ç»Ÿç»¼åˆæµ‹è¯•")
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("æ•°æ®åº“æ“ä½œæµ‹è¯•", test_database_operations()),
        ("RSSæºæµ‹è¯•", test_rss_sources()),
        ("APIæºæµ‹è¯•", test_api_sources()),
        ("å®Œæ•´å·¥ä½œæµæµ‹è¯•", test_full_workflow())
    ]
    
    results = {}
    
    for test_name, test_coro in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ§ª {test_name}")
        logger.info(f"{'='*50}")
        
        try:
            result = await test_coro
            results[test_name] = result
            
            if isinstance(result, bool):
                status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            else:
                status = f"âœ… å®Œæˆ (ç»“æœ: {result})"
            
            logger.info(f"ğŸ“Š {test_name}: {status}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name}æ‰§è¡Œå¼‚å¸¸: {e}")
            results[test_name] = False
        
        # æµ‹è¯•é—´éš”
        await asyncio.sleep(1)
    
    # æ€»ç»“
    logger.info(f"\n{'='*50}")
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    logger.info(f"{'='*50}")
    
    passed = 0
    total = len(results)
    
    for test_name, result in results.items():
        if isinstance(result, bool):
            status = "âœ… PASS" if result else "âŒ FAIL"
            if result:
                passed += 1
        else:
            status = f"âœ… PASS ({result})"
            passed += 1
        
        logger.info(f"  {test_name}: {status}")
    
    logger.info(f"\nğŸ¯ æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! æ–°é—»èšåˆç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")

if __name__ == "__main__":
    # å®‰è£…ç¼ºå¤±çš„ä¾èµ–
    try:
        import feedparser
        import newspaper
    except ImportError as e:
        logger.error(f"ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        logger.info("è¯·è¿è¡Œ: pip install feedparser newspaper3k")
        sys.exit(1)
    
    asyncio.run(main()) 