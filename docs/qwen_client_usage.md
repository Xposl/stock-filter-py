# åƒé—®å®¢æˆ·ç«¯ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æ–°çš„åƒé—®å®¢æˆ·ç«¯åŸºäºOpenAI Pythonåº“çš„è®¾è®¡æ¨¡å¼å®ç°ï¼Œä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼çš„OpenAIå…¼å®¹æ¥å£ï¼Œæä¾›ç®€æ´æ˜“ç”¨çš„APIã€‚

## ç‰¹æ€§

- ğŸ”„ **OpenAIå…¼å®¹**: é‡‡ç”¨OpenAIåº“çš„è®¾è®¡æ¨¡å¼å’Œæ¥å£é£æ ¼
- ğŸš€ **åŒæ­¥/å¼‚æ­¥**: æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§è°ƒç”¨æ–¹å¼
- ğŸ“¡ **æµå¼è¾“å‡º**: æ”¯æŒå®æ—¶æµå¼å“åº”
- ğŸ› ï¸ **ä¾¿æ·å‡½æ•°**: æä¾›ç®€åŒ–çš„ä¾¿æ·å‡½æ•°
- ğŸ¯ **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒåƒé—®ç³»åˆ—æ‰€æœ‰æ¨¡å‹
- ğŸ”§ **Function Calling**: æ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½
- ğŸ“ **å®Œæ•´ç±»å‹æç¤º**: æä¾›å®Œæ•´çš„Pythonç±»å‹æç¤º

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install openai>=1.0.0 dashscope>=1.14.0
```

### 2. è®¾ç½®APIå¯†é’¥

```bash
export DASHSCOPE_API_KEY="your_api_key_here"
```

æˆ–åœ¨ä»£ç ä¸­è®¾ç½®ï¼š

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

client = QwenClient(api_key="your_api_key_here")
```

## åŸºç¡€ç”¨æ³•

### åŒæ­¥å®¢æˆ·ç«¯

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = QwenClient()

# ç®€å•å¯¹è¯
response = client.simple_chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response)

# å¸¦ç³»ç»Ÿæç¤ºçš„å¯¹è¯
response = client.simple_chat(
    "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"
)
print(response)

# å®Œæ•´APIè°ƒç”¨
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŠ•èµ„é¡¾é—®"},
    {"role": "user", "content": "è¯·åˆ†æä¸€ä¸‹å½“å‰è‚¡å¸‚"}
]

response = client.chat_completions_create(
    messages=messages,
    temperature=0.7,
    max_tokens=500
)

print(f"å›å¤: {response.content}")
print(f"æ¨¡å‹: {response.model}")
print(f"Tokenä½¿ç”¨: {response.usage}")
```

### å¼‚æ­¥å®¢æˆ·ç«¯

```python
import asyncio
from core.ai_agents.llm_clients.qwen_client import AsyncQwenClient

async def main():
    client = AsyncQwenClient()
    
    # å¼‚æ­¥ç®€å•å¯¹è¯
    response = await client.simple_chat("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½")
    print(response)
    
    # å¼‚æ­¥å®Œæ•´APIè°ƒç”¨
    messages = [
        {"role": "user", "content": "ç”¨Pythonå†™ä¸€ä¸ªHello Worldç¨‹åº"}
    ]
    
    response = await client.chat_completions_create(
        messages=messages,
        temperature=0.3
    )
    
    print(response.content)

# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(main())
```

## æµå¼è¾“å‡º

### åŒæ­¥æµå¼

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

client = QwenClient()

messages = [
    {"role": "user", "content": "è¯·è®²ä¸€ä¸ªå…³äºAIçš„æ•…äº‹"}
]

# æµå¼å“åº”
stream_response = client.chat_completions_create(
    messages=messages,
    stream=True,
    temperature=0.8
)

print("æµå¼è¾“å‡º:")
for chunk in stream_response:
    if chunk.content:
        print(chunk.content, end="", flush=True)
print("\n")
```

### å¼‚æ­¥æµå¼

```python
import asyncio
from core.ai_agents.llm_clients.qwen_client import AsyncQwenClient

async def stream_demo():
    client = AsyncQwenClient()
    
    messages = [
        {"role": "user", "content": "è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"}
    ]
    
    stream_response = await client.chat_completions_create(
        messages=messages,
        stream=True
    )
    
    print("å¼‚æ­¥æµå¼è¾“å‡º:")
    async for chunk in stream_response:
        if chunk.content:
            print(chunk.content, end="", flush=True)
    print("\n")

asyncio.run(stream_demo())
```

## ä¾¿æ·å‡½æ•°

### åŒæ­¥ä¾¿æ·å‡½æ•°

```python
from core.ai_agents.llm_clients.qwen_client import chat, stream_chat

# ç®€å•å¯¹è¯
response = chat("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(response)

# æµå¼å¯¹è¯
print("æµå¼è¾“å‡º:")
for chunk in stream_chat("è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ "):
    print(chunk, end="", flush=True)
print("\n")
```

### å¼‚æ­¥ä¾¿æ·å‡½æ•°

```python
import asyncio
from core.ai_agents.llm_clients.qwen_client import async_chat, async_stream_chat

async def convenience_demo():
    # å¼‚æ­¥ç®€å•å¯¹è¯
    response = await async_chat("è§£é‡Šé‡å­è®¡ç®—")
    print(response)
    
    # å¼‚æ­¥æµå¼å¯¹è¯
    print("å¼‚æ­¥æµå¼è¾“å‡º:")
    async for chunk in async_stream_chat("ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼Ÿ"):
        print(chunk, end="", flush=True)
    print("\n")

asyncio.run(convenience_demo())
```

## é«˜çº§ç”¨æ³•

### ä½¿ç”¨QwenMessageå¯¹è±¡

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient, QwenMessage

client = QwenClient()

# ä½¿ç”¨QwenMessageå¯¹è±¡
messages = [
    QwenMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶"),
    QwenMessage(role="user", content="è§£é‡Šä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„")
]

response = client.chat_completions_create(messages=messages)
print(response.content)
```

### å¤šæ¨¡å‹åˆ‡æ¢

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
models = ["qwen-turbo", "qwen-plus", "qwen-max"]

for model in models:
    client = QwenClient(model=model)
    response = client.simple_chat("ä½ å¥½")
    print(f"æ¨¡å‹ {model}: {response[:50]}...")
```

### Function Callingï¼ˆå·¥å…·è°ƒç”¨ï¼‰

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

client = QwenClient()

# å®šä¹‰å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

messages = [
    {"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
]

response = client.function_call(
    messages=messages,
    tools=tools,
    model="qwen-max"
)

print(response.content)
if response.choices[0].message.tool_calls:
    print("å·¥å…·è°ƒç”¨:", response.choices[0].message.tool_calls)
```

### å…¨å±€å®¢æˆ·ç«¯

```python
from core.ai_agents.llm_clients.qwen_client import get_qwen_client, get_async_qwen_client

# è·å–å…¨å±€åŒæ­¥å®¢æˆ·ç«¯
client = get_qwen_client()
response = client.simple_chat("æµ‹è¯•å…¨å±€å®¢æˆ·ç«¯")

# è·å–å…¨å±€å¼‚æ­¥å®¢æˆ·ç«¯
async_client = get_async_qwen_client()
# response = await async_client.simple_chat("æµ‹è¯•å…¨å±€å¼‚æ­¥å®¢æˆ·ç«¯")
```

## é…ç½®é€‰é¡¹

### å®¢æˆ·ç«¯åˆå§‹åŒ–å‚æ•°

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

client = QwenClient(
    api_key="your_api_key",                    # APIå¯†é’¥
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # åŸºç¡€URL
    model="qwen-plus",                         # é»˜è®¤æ¨¡å‹
    timeout=60,                                # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=3                              # æœ€å¤§é‡è¯•æ¬¡æ•°
)
```

### æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

print("æ”¯æŒçš„æ¨¡å‹:", QwenClient.SUPPORTED_MODELS)
```

è¾“å‡ºï¼š
```
['qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-latest', 
 'qwen-long', 'qwen-vl-plus', 'qwen-vl-max', 'qwen-vl-max-latest',
 'qwen-vl-ocr-latest', 'qwen-math-plus', 'qwen-math-turbo',
 'qwen-coder-plus', 'qwen-coder-turbo', 'qwq-32b-preview']
```

## å“åº”å¯¹è±¡

### QwenResponse

```python
response = client.chat_completions_create(messages=messages)

print(f"å“åº”ID: {response.id}")
print(f"æ¨¡å‹: {response.model}")
print(f"å†…å®¹: {response.content}")
print(f"å®ŒæˆåŸå› : {response.finish_reason}")
print(f"Tokenä½¿ç”¨æƒ…å†µ: {response.usage}")
print(f"åˆ›å»ºæ—¶é—´: {response.created}")
```

### QwenStreamResponse

```python
for chunk in client.chat_completions_create(messages=messages, stream=True):
    print(f"ç‰‡æ®µID: {chunk.id}")
    print(f"å†…å®¹: {chunk.content}")
    print(f"å®ŒæˆåŸå› : {chunk.finish_reason}")
```

## é”™è¯¯å¤„ç†

```python
from core.ai_agents.llm_clients.qwen_client import QwenClient

try:
    client = QwenClient(api_key="invalid_key")
    response = client.simple_chat("æµ‹è¯•")
except ValueError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
except Exception as e:
    print(f"APIè°ƒç”¨é”™è¯¯: {e}")
```

## æœ€ä½³å®è·µ

### 1. ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶
DASHSCOPE_API_KEY=your_api_key_here
```

### 2. å¼‚æ­¥ä½¿ç”¨

å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯ï¼š

```python
import asyncio
from core.ai_agents.llm_clients.qwen_client import AsyncQwenClient

async def batch_process():
    client = AsyncQwenClient()
    
    tasks = []
    for prompt in ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]:
        task = client.simple_chat(prompt)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

### 3. æµå¼å¤„ç†é•¿æ–‡æœ¬

å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œä½¿ç”¨æµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒï¼š

```python
def generate_article(topic):
    client = QwenClient()
    
    messages = [
        {"role": "user", "content": f"è¯·å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« "}
    ]
    
    content = ""
    for chunk in client.chat_completions_create(messages=messages, stream=True):
        if chunk.content:
            content += chunk.content
            print(chunk.content, end="", flush=True)
    
    return content
```

### 4. é”™è¯¯é‡è¯•

```python
import time
from core.ai_agents.llm_clients.qwen_client import QwenClient

def robust_chat(prompt, max_retries=3):
    client = QwenClient()
    
    for attempt in range(max_retries):
        try:
            return client.simple_chat(prompt)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            print(f"é‡è¯• {attempt + 1}/{max_retries}: {e}")
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

## è¿ç§»æŒ‡å—

### ä»æ—§ç‰ˆæœ¬è¿ç§»

å¦‚æœæ‚¨ä¹‹å‰ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆæœ¬çš„QwenLLMClientï¼Œå¯ä»¥è¿™æ ·è¿ç§»ï¼š

```python
# æ—§ç‰ˆæœ¬
async with QwenLLMClient() as client:
    response = await client.chat_completion(messages)
    content = response.content

# æ–°ç‰ˆæœ¬
client = QwenClient()
response = client.chat_completions_create(messages=messages)
content = response.content
```

### å‘åå…¼å®¹

æ–°ç‰ˆæœ¬æä¾›äº†å‘åå…¼å®¹çš„åˆ«åï¼š

```python
from core.ai_agents.llm_clients.qwen_client import QwenLLMClient

# QwenLLMClient æ˜¯ QwenClient çš„åˆ«å
client = QwenLLMClient()  # ç­‰åŒäº QwenClient()
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ

A: 
- `qwen-turbo`: é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- `qwen-plus`: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨
- `qwen-max`: æœ€å¼ºæ€§èƒ½ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡

### Q: å¦‚ä½•å¤„ç†Tokené™åˆ¶ï¼Ÿ

A: ä½¿ç”¨`max_tokens`å‚æ•°æ§åˆ¶è¾“å‡ºé•¿åº¦ï¼š

```python
response = client.chat_completions_create(
    messages=messages,
    max_tokens=500  # é™åˆ¶è¾“å‡ºé•¿åº¦
)
```

### Q: å¦‚ä½•ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å‚æ•°ï¼Ÿ

A: 
- `temperature=0.0`: ç¡®å®šæ€§è¾“å‡º
- `temperature=0.7`: å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
- `temperature=1.0`: æœ€å¤§åˆ›é€ æ€§

```python
response = client.chat_completions_create(
    messages=messages,
    temperature=0.7
)
```

## æ›´å¤šèµ„æº

- [é˜¿é‡Œäº‘ç™¾ç‚¼æ§åˆ¶å°](https://bailian.console.aliyun.com/)
- [DashScope APIæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [OpenAI Pythonåº“æ–‡æ¡£](https://github.com/openai/openai-python) 